% Load the kaobook class
\documentclass[
    a4paper, % Page size
    fontsize=10pt, % Base font size
    twoside=true, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
    %open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
    %chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
    numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
    fontmethod=tex,
]{kaobook}

\ifxetexorluatex
    \usepackage{polyglossia}
    \setmainlanguage{english}
\else
    \usepackage[english]{babel}
\fi
\usepackage[english=american]{csquotes}
\setcounter{tocdepth}{1}
\usepackage{orcidlink}
\usepackage{siunitx}
\usepackage{astro}

\usepackage[backend=biber, style=numeric-comp,backref, sorting=none, firstinits=true]{biblatex}

% Hack to include Collaboration author field
\DeclareSourcemap{
 \maps[datatype=bibtex,overwrite=true]{
  \map{
    \step[fieldsource=Collaboration, final=true]
    \step[fieldset=usera, origfieldval, final=true]
  }
 }
}

\renewbibmacro*{author}{%
  \iffieldundef{usera}{%
    \printnames{author}%
  }{%
    \printfield{usera}, \printnames{author}%
  }%
}%

\usepackage{kaobiblio}
\addbibresource{thesis.bib}

\usepackage[framed=true]{kaotheorems}
\usepackage{kaorefs}

\graphicspath{{figures/}{images/}}

\makeindex[columns=3, title=Alphabetical Index, intoc]


\begin{document}

%----------------------------------------------------------------------------------------
%   BOOK INFORMATION
%----------------------------------------------------------------------------------------
\subject{Doctoral Thesis}
% \titlehead{Optical Follow-Up of High-Energy Neutrinos}
\title[Optical Follow-Up of High-Energy Neutrinos]{Optical Follow-Up of High-Energy Neutrinos}
\author[SR]{Simeon Reusch}% \orcidlink{0000-0002-7788-628X}}
\date{\today}
\publishers{Humboldt-Universit√§t zu Berlin}

%----------------------------------------------------------------------------------------

\frontmatter

\makeatletter
\uppertitleback{\@titlehead}

\lowertitleback{
    \textbf{Copyright} \\
    \cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work. To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0}
    
    \medskip
 
    This thesis was typeset with the help of \href{https://sourceforge.net/projects/koma-script}{\KOMAScript} and \href{https://www.latex-project.org}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook}{kaobook} class.
    
    \medskip

    The code used to typeset this thesis and create the figures within can be accessed at \href{https://github.com/simeonreusch/koma-thesis}{github.com/simeonreusch/thesis}

    \medskip
    
    \textbf{Publisher} \\
    First published in August 2023 by \@publishers
}
\makeatother


\maketitle

\begingroup % Local scope for the following commands

% Define the style for the TOC, LOF, and LOT
%\setstretch{1} % Uncomment to modify line spacing in the ToC
%\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
\setlength{\textheight}{230\vscale} % Manually adjust the height of the ToC pages

% Turn on compatibility mode for the etoc package
%\etocstandarddisplaystyle % "toc display" as if etoc was not loaded
%\etocstandardlines % "toc lines as if etoc was not loaded

\tableofcontents
\listoffigures

\let\cleardoublepage\bigskip
\let\clearpage\bigskip

\listoftables

\endgroup

\mainmatter
\setchapterstyle{kao}

%\chapter{Theoretical background}
%\addpart{Title of the Part}
\pagelayout{margin}



\input{chapters/icecube.tex}
\input{chapters/ztf.tex}
\setchapterimage[7cm]{fu/heading3.png}
\chapter{The ZTF Follow-Up Pipeline} 
\labch{fupipeline}
After introducing the IceCube detector (see chapter \ref{ic}) and the Zwicky Transient Facility (see chapter \ref{ztf}), we now have all the ingredients to introduce the ZTF high-energy neutrino follow-up pipeline.

IceCube sends out \num{\sim2.2} astrophysical high-energy neutrino alerts on average per month (see section \ref{ic_alerts}). Due to its large FoV and fully robotic operation, ZTF is the ideal follow-up instrument for these alerts. The typically reported IceCube localization regions can be covered with only one pointing of the telescope.

The follow-up procedure can be outlined as follows: An \textit{IceCube alert} is received. If the alert meets our \textit{alert quality criteria}, we do an \textit{observability check}. If the sky region is accessible to ZTF, we \textit{observe}. After observations, we \textit{filter} candidates with \texttt{AMPEL} \sidecite{Nordin2019}, followed by visual inspection and -- if needed -- trigger \textit{additional follow-up}.

\section{Alert Cuts}\label{alert_cuts}
As we only have limited telescope time, we apply quality cuts on the high-energy neutrino alerts received via GCN. As outlined in section \ref{ic_event_selection}, there are two alert streams: \textit{Gold alerts} with an average purity of \SI{50}{\percent} (\SI{36}{\percent} of all non-retracted alerts), and \textit{bronze alerts} with an average purity of \SI{30}{\percent} (\SI{64}{\percent}).

In general, we only follow up if the reported rectangular \SI{90}{\percent} uncertainty region is smaller than \SI{40}{\square\deg}. All gold alerts making this cut qualify for follow up. There is a more stringent cut on the uncertainty area of bronze alerts, requiring these are $<\SI{10}{\square\deg}$. To avoid contamination by foreground stars, we implement a cut on galactic latitude of $|b|>\SI{10}{\degree}$.

\section{Observation Planning with \texttt{planobs}}\label{planobs}
If an alert makes these cuts, one needs to check if observations with ZTF are feasible. To reduce the potential for errors, this is done with assistance by the \texttt{planobs} \sidecite{Reusch2023} tool, developed mainly by the author to automate the task as much as possible.

\texttt{planobs} is written in Python, and is deployed on a virtual private server. The backend runs on \texttt{Flask}\sidenote{\url{https://flask.palletsprojects.com}} behind a \texttt{nginx}\sidenote{\url{https://nginx.com}} reverse proxy and serves a Slack\sidenote{\url{https://slack.com/}} bot integrated into the DESY multimessenger group chat.

The tool is constructed to be used with a simple command-line-style interface in the Slack chat. For example, 
\begin{lstlisting}[language=bash,style=kaolstplain]
Plan IC221223A -multiday
\end{lstlisting}
will create an observation plan for IceCube high-energy neutrino IC220501A, spanning multiple days.

To obtain the positional and error information on the neutrino in question from the respective GCN circular, \texttt{planobs} is searching for the GCN on an experimental server\sidenote{\url{https://heasarc.gsfc.nasa.gov/wsgi-scripts/tach/gcn_v2/tach.wsgi}}. As notices are written by humans, it fuzzily parses them to extract the relevant information. After this, observability at Mt. Palomar is calculated, defaulting to the current time (optionally, a desired observation time can be requested). 

\begin{figure}[h!]
    \includegraphics{fu/planobs_airmass.pdf}
    \caption[Observation plan]{Observation plot created by \texttt{planobs} for the follow-up of IceCube neutrino IC221223A. The altitude and airmass of the alert region at Mt. Palomar are shown in blue. The red and green shaded regions are proposed observation windows in the \textit{r}- and \textit{r}-band. The red arrows show the airmass limit of 2.0, and the moon is displayed as yellow dotted curve. The gray shaded region marks night-time at the telescope site. All information automatically extracted from the GCN circular are shown on top.}
    \labfig{planobs}
\end{figure}
\begin{marginfigure}
    \includegraphics{fu/planobs_grid.pdf}
    \caption[\texttt{planobs} ZTF grid]{The \SI{90}{\percent} uncertainty rectangle of IC221223A overlayed onto the ZTF grid.}
    \labfig{planobs_grid}
\end{marginfigure}
An example observability plot is shown in Fig. \ref{fig:planobs}. The blue curve shows the altitude of the sky region in question above Mt. Palomar, and the red and green shaded regions mark the two proposed observation windows in the \textit{r}- and the \textit{g}-band.

Additionally, for each field within the primary and secondary grid (see section \ref{ztf_grid}) that have overlap with the uncertainty region\sidenote{There is an additional check to ensure the fields have reference images available, see section \ref{ztf_image_subtraction}.}, the coverage is calculated and the field with the highest coverage is selected. Fig. \ref{fig:planobs_grid} shows such an overlay plot for IC221223A and ZTF field 693.

If the plan looks good, one needs to invoke
\begin{lstlisting}[language=bash,style=kaolstplain]
Plan IC221223A -trigger
\end{lstlisting} 
to submit the observation request via a dedicated API to the telescope scheduler. There is additional functionality to ensure that the trigger has been added to the telescope queue. If all goes well and weather permits, the observations are carried out, and candidate vetting can begin.

As we are interested in the evolution of potential source candidates, usually observations within a \SI{10}{\day} window are triggered. To obtain deep images during the first night, we trigger \SI{300}{\second} exposures in the \textit{g}- and \textit{r}-band in the first night. These are followed by shallower \SI{30}{\second} observations in the \textit{g}-band during nights 2, 3, 5 and 7, and finally shallow observations in both \textit{g}- and \textit{r}-band during night 9.

\section{The \texttt{AMPEL} Broker} \label{ampel}
The next step in the pipeline is the selection of good candidates. ZTF typically serves 200000 alerts per night. As only a fraction of those is relevant for the neutrino follow up, we need software to cut down the number of alerts. This is exactly what AMPEL is doing, a streaming data analysis framework developed at Humboldt-University Berlin and DESY Zeuthen with contribution of the author.

The main design goals of \texttt{AMPEL} comprise scalability, modularity and provenance tracking. It was built with the data rate of future Rubin observatory \sidecite{Ivezic2019} in mind, and was subsequently selected as one of 7 brokers\sidenote{See \url{https://www.lsst.org/scientists/alert-brokers}.} for Rubin observatory. The complete software stack is written in Python.


\begin{figure}[h!]
    \includegraphics{fu/ampel_design.pdf}
    \caption[\texttt{AMPEL} overview]{Overview of the \texttt{AMPEL} data processing. Alerts from DiRAC are ingested into \texttt{AMPEL}, where they are processed, combined, analyzed and served to science consumers. From \cite{Nordin2019}}
    \labfig{ampel_design}
\end{figure}

Fig. \ref{fig:ampel_design} shows the design and information flow of \texttt{AMPEL}. On the top, data from Mt. Palomar is transmitted to IPAC (see section \ref{ztf_data_link}), where detections are extracted from the difference images. These are then sent to the Institute for Data Intensive Research in Astrophysics \& Cosmology (DiRAC) at the University of Washington, where they are distributed via parallel Kafka streams. This is the live data stream \texttt{AMPEL} listens to.

The first of several execution layers (\textit{tiers}) is the \textbf{Filtering} stage (Tier 0). Here different filters can be implemented, reducing the large number of alerts by different criteria. These comprise e.g. \texttt{RealBogus} and \texttt{sgscore} (see section \ref{ztf_image_subtraction}), color evolution, host galaxy properties and the (non-) existence of detection history. All alerts surviving the filtering stage are then stored in a \texttt{MongoDB}\sidenote{\url{https://mongodb.com}} database collection. Additionally, processing states are also stored in another collection.

\textbf{Tier 1} can be skipped here, as it serves technical purposes. The next relevant stage is the \textbf{Light curve nalysis} stage (Tier 2). Here, additional information on the transients are either obtained or generated. Possible steps are querying external catalogs for host galaxy or redshift information, fitting lightcurves with various models or ML-based photometric classification.

The last level, tier 3, executes \textbf{Population analysis}. These are schedulable actions, triggered on request or at pre-defined times (ranging from yearly data dumps, through daily updates to nearly real-time execution). A typical use case is the automated ranking of different transients for a specific science goal. An example is the daily posting of new supernova candidates to a Slack channel, ranked by how promising they are for spectroscopic follow up \cite{Nordin2019}.

To simplify matters and allow reprocessing as well as full replayability, all ZTF alerts received via the Kafka streams since June 2018 are also stored in an archival alert database hosted at DESY\sidenote{\url{https://ampel.zeuthen.desy.de/api/ztf/archive/v3/docs}}. The database is \texttt{Postgresql}\sidenote{\url{https://postgresql.org/}} and can be accessed with a web frontend and an API.

\section{Candidate Filtering with \texttt{nuztf}}
To streamline the neutrino follow-up process, \texttt{nuztf} \sidecite{Stein2023} was created for filtering and inspecting candidate counterparts, with heavy contribution by the author. It is written in Python, and relies heavily on \texttt{AMPEL} (section \ref{ampel}).

When run, it executes the following steps: It obtains the IceCube alert information from the GCN, constructs a \texttt{HEALPix} map from the \SI{90}{\percent} uncertainty rectangle, queries the \texttt{AMPEL} archive for all alerts within the \texttt{HEALPix} map\sidenote{Because the archive database contains \texttt{HEALPix} indices with different resolutions, this kind of query is much faster than a cone search.} in a given time range, applies the \texttt{AMPEL} \texttt{DecentFilter} \cite{Nordin2019} with custom parameters (see below), crossmatches the surviving counterpart candidates to a list of catalogs, and finally creates an overview pdf file with details and a light curve for each candidate.

\subsection{\texttt{DecentFilter} parameters}
The GCN parsing is done akin to \texttt{planobs} (see section \ref{planobs}). After extraction of the alert information and querying the Archive database with a \texttt{HEALPix} map, \texttt{DecentFilter} is run with the following parameters:
\begin{description}
    \item[Time window] The transient must have shown activity in a \SI{14}{\day} window after neutrino detection
    \item[\texttt{RealBogus}] The transient must have a \texttt{RealBogus} score of $>0.3$.
    \item[Positive subtraction] Sometimes, subtraction from reference images result in negative flux. This criterion ensures that there is excess flux.
    \item[Detections] The candidate must have at least 2 detections, separated by at least \SI{15}{\minute} (note that this needs to be reflected in the observation planning. \texttt{planobs} (section \ref{planobs}) takes care of that.
    \item[\texttt{sgscore} veto] Vetoes any object with an \texttt{sgscore}$>0.8$, i.e. if it has a high probability of being a star
    \item[Maximum distance to PS1] The \texttt{sgscore} star-galaxy classifier is trained on PS1. This criterion enforces a maximum distance to the PS1 source of $<\SI{1}{\degree}$. If that is exceeded, the association between alert and PS1 source is not secure and the \texttt{sgscore} veto is ignored. The veto is also ignored if 3 PS1 sources are closer than \SI{3}{\degree}, with a \texttt{sgscore} lying between $0.4$ and $0.6$, as this hints at possible PS1 source confusion.
    \item[Gaia star veto] Vetoes the source if the probability of it being a star is high, based on parameters from the Gaia survey.
\end{description}

\subsection{Catalog crossmatch}
All surviving candidates are then spatially crossmatched to a set of catalogs.


%\chapter{Candidate TDE AT2019fdr: a possible source?}
%\chapter{The ZTF nuclear sample}
%\chapter{Conclusion and Outlook}
\input{chapters/appendix.tex}

%----------------------------------------------------------------------------------------

\backmatter % Denotes the end of the main document content
\setchapterstyle{plain} % Output plain chapters from this point onwards 

%----------------------------------------------------------------------------------------
%   BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% The bibliography needs to be compiled with biber using your LaTeX editor, or on the command line with 'biber main' from the template directory
%\defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\printbibliography[heading=bibintoc, title=Bibliography] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note


\printindex % Output the index

\end{document}
